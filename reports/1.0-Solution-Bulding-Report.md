# Solution Building Report

## Baseline: Dictionary based

We established a baseline approach utilizing a dictionary-based method, where a list of known toxic words and phrases were replaced with non-toxic alternatives. The principal shortcomings of this method were:

- **Lack of Context**: It didn't take into account the surrounding context of words, which could result in unnatural or incorrect replacements.
- **Limited Vocabulary**: Given the nuanced nature of toxicity, which often relies on context, an exhaustive list of toxic terms is impractical to maintain.
- **Scalability Issues**: Keeping the dictionary updated with all potential toxic words is an onerous task.

Acknowledging these drawbacks prompted us to seek out more sophisticated machine learning solutions.

## Hypothesis 1: Using Bert-to-Bert model using Hugging Face

Initially, Hypothesis 1 proposed the use of a Bert-to-Bert model facilitated by Hugging Face's transformative capabilities. However, despite its promising potential for solving the problem, the complexity of the model paired with the constraints of my hardware - an Apple M1 Pro with 16GB RAM and MPS - made it unfeasible, as the library doesn't yet fully support this set of hardware. My limited experience in training and fine-tuning models using Hugging Face further compounded the issue. Therefore, we could not realize this hypothesis in practice.

Relevant code and preprocessing steps for this hypothesis can be referenced in `2.0-preprocessing-BERT-to-BERT.ipynb`, and the model training attempts in `3.0-model-training-BERT-to-BERT.ipynb`.

## Hypothesis 2: Using an LSTM based Seq2Seq model using PyTorch

For our second hypothesis, we pivoted towards a more familiar architecture: an LSTM-based Seq2Seq model built using PyTorch. This approach aligned better with my prior experience and was more manageable to implement given the hardware at my disposal.

### Model Architecture

The architecture comprised two primary components:

- **Encoder**: An LSTM network that processes the input text and compresses the information into a context vector.
- **Decoder**: Another LSTM network that uses the context vector to generate the detoxified output sequence.

We implemented the model with the following specifications:

- **Input Dimension**: Size of the input vocabulary
- **Output Dimension**: Size of the output vocabulary
- **Embedding Dimensions**: 128 for both encoder and decoder
- **Hidden Dimensions**: 512, representing the size of the LSTM's hidden states
- **Number of Layers**: 2, to allow the model to learn more complex representations
- **Dropout**: Set at 0.5 for regularization to prevent overfitting

This architecture offered a balance between complexity and performance, allowing us to train a model capable of understanding and transforming the nuances of natural language without the need for excessive computational resources.

For more detailed insights into the implementation and results, please consult the code documented within `model_utils.py`.

---

This report aims to provide a transparent view of the iterative process and rationale behind the chosen solutions. Our exploration underlines the intricate balance between theoretical model choice and practical execution constraints.
