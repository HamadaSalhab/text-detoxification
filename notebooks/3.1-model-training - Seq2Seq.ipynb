{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/hamadasalhab/Library/CloudStorage/OneDrive-АНОВОУниверситетИннополис/Disk D/Innopolis Study Materials/F23/PMLDL/Assignments/Assignment#01/text-detoxification/src/data/../../data/raw/filtered_paranmt.zip\n",
      "All set. The dataset can be found in project_root_dir/data/raw.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from src.data.make_dataset import get_dataset\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = '../data/raw/filtered_paranmt/filtered.tsv'\n",
    "VOCABS_DIR = '../data/interim/'\n",
    "REFERENCE_VOCAB_PATH = VOCABS_DIR + 'reference_vocab.pkl'\n",
    "TRANSLATION_VOCAB_PATH = VOCABS_DIR + 'translation_vocab.pkl'\n",
    "\n",
    "# Load data\n",
    "get_dataset()\n",
    "df = pd.read_csv(DATASET_PATH, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# Define special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "# Build vocabularies\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    # Count the frequencies of tokens in the texts\n",
    "    counter = Counter(chain.from_iterable(texts))\n",
    "    # Create the vocabulary mapping each token to a unique index\n",
    "    vocab = {token: idx for idx, (token, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    # Add special tokens to the beginning of the dictionary\n",
    "    vocab = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, **vocab}\n",
    "    return vocab\n",
    "\n",
    "# Tokenization and numericalization\n",
    "def tokenize_and_numericalize(text, vocab):\n",
    "    tokens = text.split()\n",
    "    numericalized = [vocab[SOS_TOKEN]] + [vocab.get(token, vocab[PAD_TOKEN]) for token in tokens] + [vocab[EOS_TOKEN]]\n",
    "    return numericalized\n",
    "\n",
    "# Tokenize texts\n",
    "reference_tokenized = df['reference'].str.split().tolist()\n",
    "translation_tokenized = df['translation'].str.split().tolist()\n",
    "\n",
    "# Build vocabularies\n",
    "reference_vocab = build_vocab(reference_tokenized)\n",
    "translation_vocab = build_vocab(translation_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from src.data.make_dataset import get_dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "INTERIM_DATASET_PATH = '../data/interim/preprocessed_new.tsv'\n",
    "\n",
    "df = pd.read_csv(INTERIM_DATASET_PATH, delimiter='\\t')\n",
    "df = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_numericalized</th>\n",
       "      <th>translation_numericalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>[1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...</td>\n",
       "      <td>[1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>[1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]</td>\n",
       "      <td>[1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254505</th>\n",
       "      <td>[1, 944, 1266, 186, 143, 572, 572, 157579, 2]</td>\n",
       "      <td>[1, 72, 20, 619, 45, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451186</th>\n",
       "      <td>[1, 629, 352, 10, 116913, 581, 2]</td>\n",
       "      <td>[1, 303, 61, 109, 4119, 437, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191213</th>\n",
       "      <td>[1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...</td>\n",
       "      <td>[1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reference_numericalized  \\\n",
       "57809   [1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...   \n",
       "132693   [1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]   \n",
       "254505      [1, 944, 1266, 186, 143, 572, 572, 157579, 2]   \n",
       "451186                  [1, 629, 352, 10, 116913, 581, 2]   \n",
       "191213  [1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...   \n",
       "\n",
       "                                translation_numericalized  \n",
       "57809       [1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]  \n",
       "132693        [1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]  \n",
       "254505                            [1, 72, 20, 619, 45, 2]  \n",
       "451186                    [1, 303, 61, 109, 4119, 437, 2]  \n",
       "191213  [1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test Split\n",
    "train, eval = train_test_split(df, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_numericalized</th>\n",
       "      <th>translation_numericalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>[1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...</td>\n",
       "      <td>[1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>[1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]</td>\n",
       "      <td>[1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254505</th>\n",
       "      <td>[1, 944, 1266, 186, 143, 572, 572, 157579, 2]</td>\n",
       "      <td>[1, 72, 20, 619, 45, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451186</th>\n",
       "      <td>[1, 629, 352, 10, 116913, 581, 2]</td>\n",
       "      <td>[1, 303, 61, 109, 4119, 437, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191213</th>\n",
       "      <td>[1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...</td>\n",
       "      <td>[1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reference_numericalized  \\\n",
       "57809   [1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...   \n",
       "132693   [1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]   \n",
       "254505      [1, 944, 1266, 186, 143, 572, 572, 157579, 2]   \n",
       "451186                  [1, 629, 352, 10, 116913, 581, 2]   \n",
       "191213  [1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...   \n",
       "\n",
       "                                translation_numericalized  \n",
       "57809       [1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]  \n",
       "132693        [1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]  \n",
       "254505                            [1, 72, 20, 619, 45, 2]  \n",
       "451186                    [1, 303, 61, 109, 4119, 437, 2]  \n",
       "191213  [1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def string_to_list(s):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        return [int(item) for item in s.strip('[]').split(', ')]\n",
    "\n",
    "class TextDetoxDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.reference_numericalized = dataframe['reference_numericalized'].apply(string_to_list)\n",
    "        self.translation_numericalized = dataframe['translation_numericalized'].apply(string_to_list)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reference_numericalized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.reference_numericalized.iloc[idx]\n",
    "        labels = self.translation_numericalized.iloc[idx]\n",
    "\n",
    "        return {\n",
    "            \"reference\": input_ids,\n",
    "            \"translation\": labels\n",
    "        }\n",
    "\n",
    "train_dataset = TextDetoxDataset(train)\n",
    "eval_dataset = TextDetoxDataset(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    reference = pad_sequence([torch.tensor(item[\"reference\"], dtype=torch.long) for item in batch],\n",
    "                                   batch_first=True, padding_value=0)\n",
    "    translation = pad_sequence([torch.tensor(item[\"translation\"], dtype=torch.long) for item in batch],\n",
    "                                batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        \"reference\": reference.to(device),\n",
    "        \"translation\": translation.to(device)\n",
    "    }\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "# Adjustments in the Seq2Seq model to accommodate GRUs\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamadasalhab/Library/CloudStorage/OneDrive-АНОВОУниверситетИннополис/Disk D/Innopolis Study Materials/F23/PMLDL/Assignments/Assignment#01/text-detoxification/pyenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(reference_vocab)\n",
    "OUTPUT_DIM = len(translation_vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LR = 0.01\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# don't forget to put the model to the right device\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the PAD token\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(iterator, desc='Training', leave=False)\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        src = batch['reference']\n",
    "        trg = batch['translation']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(iterator, desc='Evaluating', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            src = batch['reference']\n",
    "            trg = batch['translation']\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 269/5778 [01:21<24:15,  3.79it/s, loss=nan]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    eval_loss = evaluate(model, eval_dataloader, criterion)\n",
    "    \n",
    "    last_loss = train_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {eval_loss:.3f} |  Val. PPL: {math.exp(eval_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': last_loss,  # Now 'last_loss' is defined\n",
    "}, 'checkpoint.pth')\n",
    "\n",
    "# Load everything (assumes model and optimizer are already instantiated)\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']  # This will now have the value from the last batch of the last epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the entire model\n",
    "# model = torch.load('model.pth')\n",
    "\n",
    "# # Load only the model state dictionary\n",
    "# # First, instantiate your model architecture\n",
    "# model = Seq2Seq(encoder, decoder, device)\n",
    "# # Then load the state dictionary\n",
    "# model.load_state_dict(torch.load('model_state_dict.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
