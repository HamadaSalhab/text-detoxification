{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABS_DIR = '../data/interim/'\n",
    "REFERENCE_VOCAB_PATH = VOCABS_DIR + 'reference_vocab.pkl'\n",
    "TRANSLATION_VOCAB_PATH = VOCABS_DIR + 'translation_vocab.pkl'\n",
    "MODELS_DIR = '../models/'\n",
    "SEQ2SEQ_LSTMS_PATH = MODELS_DIR + \"seq2seq - lstms.pth\"\n",
    "SEQ2SEQ_GRUS_PATH = MODELS_DIR + \"seq2seq - grus.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire model\n",
    "model = torch.load('model.pth')\n",
    "\n",
    "# Load only the model state dictionary\n",
    "# First, instantiate your model architecture\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "# Then load the state dictionary\n",
    "model.load_state_dict(torch.load('model_state_dict.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the sentence, add the <sos> and <eos> tokens, and numericalize\n",
    "    tokens = [src_vocab.get(token, src_vocab[PAD_TOKEN]) for token in sentence.split()]\n",
    "    numericalized_tokens = [src_vocab[SOS_TOKEN]] + tokens + [src_vocab[EOS_TOKEN]]\n",
    "    \n",
    "    # Convert to Tensor and add a batch dimension\n",
    "    src_tensor = torch.LongTensor(numericalized_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict the target sequence\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        trg_indexes = [trg_vocab[SOS_TOKEN]]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "            with torch.no_grad():\n",
    "                output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "            # Get the predicted next token (the one with the highest probability)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # If the <eos> token is predicted, stop\n",
    "            if pred_token == trg_vocab[EOS_TOKEN]:\n",
    "                break\n",
    "    \n",
    "    # Convert the predicted numerical tokens to words\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(idx)] for idx in trg_indexes]\n",
    "    \n",
    "    # Return the words after the <sos> token\n",
    "    return trg_tokens[1:-1]\n",
    "\n",
    "# Load vocabularies\n",
    "with open(REFERENCE_VOCAB_PATH, 'rb') as f:\n",
    "    src_vocab = pickle.load(f)\n",
    "with open(TRANSLATION_VOCAB_PATH, 'rb') as f:\n",
    "    trg_vocab = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this sentence if you want to make another prediction\n",
    "src_sentence = \"I'm not the fucking best guy in this whole shitty universe\"\n",
    "detoxified_tokens = detoxify_sentence(src_sentence, src_vocab, trg_vocab, model, device)\n",
    "print(\" \".join(detoxified_tokens))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
