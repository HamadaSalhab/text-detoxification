{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Notebook\n",
    "\n",
    "Model used: Sequence-to-Sequence with Encoder and Decoder as LSTM (Long Short-Term Memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Necessary Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_preprocessed_data import load_vocabs\n",
    "\n",
    "reference_vocab, translation_vocab = load_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_preprocessed_data import get_dataframe\n",
    "\n",
    "df = get_dataframe(sample_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_numericalized</th>\n",
       "      <th>translation_numericalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>[1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...</td>\n",
       "      <td>[1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>[1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]</td>\n",
       "      <td>[1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254505</th>\n",
       "      <td>[1, 944, 1266, 186, 143, 572, 572, 157579, 2]</td>\n",
       "      <td>[1, 72, 20, 619, 45, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451186</th>\n",
       "      <td>[1, 629, 352, 10, 116913, 581, 2]</td>\n",
       "      <td>[1, 303, 61, 109, 4119, 437, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191213</th>\n",
       "      <td>[1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...</td>\n",
       "      <td>[1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reference_numericalized  \\\n",
       "57809   [1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...   \n",
       "132693   [1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]   \n",
       "254505      [1, 944, 1266, 186, 143, 572, 572, 157579, 2]   \n",
       "451186                  [1, 629, 352, 10, 116913, 581, 2]   \n",
       "191213  [1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...   \n",
       "\n",
       "                                translation_numericalized  \n",
       "57809       [1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]  \n",
       "132693        [1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]  \n",
       "254505                            [1, 72, 20, 619, 45, 2]  \n",
       "451186                    [1, 303, 61, 109, 4119, 437, 2]  \n",
       "191213  [1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data to Training & Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test Split\n",
    "train, eval = train_test_split(df, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_numericalized</th>\n",
       "      <th>translation_numericalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>[1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...</td>\n",
       "      <td>[1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>[1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]</td>\n",
       "      <td>[1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254505</th>\n",
       "      <td>[1, 944, 1266, 186, 143, 572, 572, 157579, 2]</td>\n",
       "      <td>[1, 72, 20, 619, 45, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451186</th>\n",
       "      <td>[1, 629, 352, 10, 116913, 581, 2]</td>\n",
       "      <td>[1, 303, 61, 109, 4119, 437, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191213</th>\n",
       "      <td>[1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...</td>\n",
       "      <td>[1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reference_numericalized  \\\n",
       "57809   [1, 2510, 2815, 623, 10, 62476, 92, 290, 991, ...   \n",
       "132693   [1, 600, 10, 601, 620, 373, 236, 1998, 86, 0, 2]   \n",
       "254505      [1, 944, 1266, 186, 143, 572, 572, 157579, 2]   \n",
       "451186                  [1, 629, 352, 10, 116913, 581, 2]   \n",
       "191213  [1, 95, 199, 182, 97, 67, 86, 4025, 1674, 791,...   \n",
       "\n",
       "                                translation_numericalized  \n",
       "57809       [1, 1344, 79, 22, 46220, 88, 113, 257, 52, 2]  \n",
       "132693        [1, 436, 10, 62, 125, 36, 585, 74, 3993, 2]  \n",
       "254505                            [1, 72, 20, 619, 45, 2]  \n",
       "451186                    [1, 303, 61, 109, 4119, 437, 2]  \n",
       "191213  [1, 130, 261, 25, 227, 22, 70525, 130, 5313, 2...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data.text_detox_data import TextDetoxDataset\n",
    "\n",
    "train_dataset = TextDetoxDataset(train)\n",
    "eval_dataset = TextDetoxDataset(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.text_detox_data import TextDetoxDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def text_detox_collate_fn(batch):\n",
    "    reference = pad_sequence([torch.tensor(item[\"reference\"], dtype=torch.long) for item in batch],\n",
    "                                   batch_first=True, padding_value=0)\n",
    "    translation = pad_sequence([torch.tensor(item[\"translation\"], dtype=torch.long) for item in batch],\n",
    "                                batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        \"reference\": reference.to(device),\n",
    "        \"translation\": translation.to(device)\n",
    "    }\n",
    "\n",
    "train_dataloader = TextDetoxDataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=text_detox_collate_fn)\n",
    "eval_dataloader = TextDetoxDataLoader(eval_dataset, batch_size=4, shuffle=False, collate_fn=text_detox_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamadasalhab/Library/CloudStorage/OneDrive-АНОВОУниверситетИннополис/Disk D/Innopolis Study Materials/F23/PMLDL/Assignments/Assignment#01/text-detoxification/pyenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_utils import TextDetoxEncoder, TextDetoxDecoder, TextDetoxSeq2SeqModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "INPUT_DIM = len(reference_vocab)\n",
    "OUTPUT_DIM = len(translation_vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = TextDetoxEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = TextDetoxDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# don't forget to put the model to the right device\n",
    "model = TextDetoxSeq2SeqModel(enc, dec, device).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the PAD token\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 5.950 | Train PPL: 383.619\n",
      "\t Val. Loss: 6.201 |  Val. PPL: 493.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from src.models.model_utils import train_detox_model, evaluate_detox_model\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_detox_model(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    eval_loss = evaluate_detox_model(model, eval_dataloader, criterion)\n",
    "    \n",
    "    last_loss = train_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {eval_loss:.3f} |  Val. PPL: {math.exp(eval_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/SEQ2SEQ_LSTMs.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save:\n",
    "Uncomment and run the next cell to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': last_loss,\n",
    "}, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load:\n",
    "Uncomment and run the next cell to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# VOCABS_DIR = '../data/interim/'\n",
    "# REFERENCE_VOCAB_PATH = VOCABS_DIR + 'reference_vocab.pkl'\n",
    "# TRANSLATION_VOCAB_PATH = VOCABS_DIR + 'translation_vocab.pkl'\n",
    "\n",
    "# # Load vocabularies\n",
    "# with open(REFERENCE_VOCAB_PATH, 'rb') as f:\n",
    "#     reference_vocab = pickle.load(f)\n",
    "# with open(TRANSLATION_VOCAB_PATH, 'rb') as f:\n",
    "#     translation_vocab = pickle.load(f)\n",
    "\n",
    "# # Define model\n",
    "# INPUT_DIM = len(reference_vocab)\n",
    "# OUTPUT_DIM = len(translation_vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# HID_DIM = 512\n",
    "# N_LAYERS = 2\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "# encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "# decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# # don't forget to put the model to the right device\n",
    "# model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the PAD token\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# checkpoint = torch.load(MODEL_PATH)\n",
    "# model = Seq2Seq(encoder, decoder, device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def detoxify_sentence(sentence, reference_vocab, translation_vocab, model, device, max_len=50):\n",
    "    # Define special tokens that will be used in the data preprocessing\n",
    "    PAD_TOKEN = '<pad>'  # Token used for padding sentences to the same length\n",
    "    SOS_TOKEN = '<sos>'  # Start-of-sentence token\n",
    "    EOS_TOKEN = '<eos>'  # End-of-sentence token\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the sentence, add the <sos> and <eos> tokens, and numericalize\n",
    "    tokens = [reference_vocab.get(token, reference_vocab[PAD_TOKEN]) for token in sentence.split()]\n",
    "    numericalized_tokens = [reference_vocab[SOS_TOKEN]] + tokens + [reference_vocab[EOS_TOKEN]]\n",
    "    \n",
    "    # Convert to Tensor and add a batch dimension\n",
    "    src_tensor = torch.LongTensor(numericalized_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict the target sequence\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        trg_indexes = [translation_vocab[SOS_TOKEN]]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "            with torch.no_grad():\n",
    "                output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "            # Get the predicted next token (the one with the highest probability)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # If the <eos> token is predicted, stop\n",
    "            if pred_token == translation_vocab[EOS_TOKEN]:\n",
    "                break\n",
    "    \n",
    "    # Convert the predicted numerical tokens to words\n",
    "    trg_tokens = [list(translation_vocab.keys())[list(translation_vocab.values()).index(idx)] for idx in trg_indexes]\n",
    "    \n",
    "    # Return the words after the <sos> token\n",
    "    return trg_tokens[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a criminal.\n"
     ]
    }
   ],
   "source": [
    "# Change this sentence if you want to make another prediction\n",
    "src_sentence = \"This assignment is fucking difficult\"\n",
    "detoxified_tokens = detoxify_sentence(src_sentence, reference_vocab, translation_vocab, model, device)\n",
    "print(\" \".join(detoxified_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57778/57778 [35:36<00:00, 27.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 0.039391276526298424\n",
      "Average ROUGE-1 score: 0.22633753009957291\n",
      "Average ROUGE-L score: 0.21869081536854712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ast\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "inverse_reference_vocab = {v: k for k, v in reference_vocab.items()}\n",
    "inverse_translation_vocab = {v: k for k, v in translation_vocab.items()}\n",
    "\n",
    "# Your model's prediction function\n",
    "def predict(sentence, reference_vocab, translation_vocab, model, device):\n",
    "    # Insert the detoxify_sentence function here\n",
    "    return detoxify_sentence(sentence, reference_vocab, translation_vocab, model, device)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Make sure the model is in evaluation mode\n",
    "bleu_scores = []\n",
    "rouge_scores = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "\n",
    "for index, row in tqdm(eval.iterrows(), total=eval.shape[0]):\n",
    "    reference_numericalized = ast.literal_eval(row['reference_numericalized'])\n",
    "    translation_numericalized = ast.literal_eval(row['translation_numericalized'])\n",
    "\n",
    "    \n",
    "    reference_text = ' '.join([inverse_reference_vocab[token] for token in reference_numericalized if token not in (0, 1, 2)])  # exclude pad, sos, eos\n",
    "    ground_truth_text = ' '.join([inverse_translation_vocab[token] for token in translation_numericalized if token not in (0, 1, 2)])  # exclude pad, sos, eos\n",
    "    \n",
    "    predicted_text_tokens = predict(reference_text, reference_vocab, translation_vocab, model, device)\n",
    "    predicted_text = ' '.join(predicted_text_tokens)\n",
    "    \n",
    "    # BLEU score\n",
    "    bleu_score = sentence_bleu([ground_truth_text.split()], predicted_text.split(), smoothing_function=SmoothingFunction().method1)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # ROUGE scores\n",
    "    scores = rouge_scores.score(ground_truth_text, predicted_text)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate average scores\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "average_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "\n",
    "print(f'Average BLEU score: {average_bleu}')\n",
    "print(f\"Average ROUGE-1 score: {average_rouge1}\")\n",
    "print(f\"Average ROUGE-L score: {average_rougeL}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
